{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179}],"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing required libraries and modules\nimport torch  # Core library for deep learning\nimport torch.nn as nn  # Provides building blocks for neural networks\nimport torch.nn.functional as F  # Commonly used functions like activation functions\nimport numpy as np  # Library for numerical operations\nimport pandas as pd  # For handling data in tabular format\nimport matplotlib.pyplot as plt  # For visualizing data\nfrom timm import create_model, list_models  # Pretrained models and utilities\nfrom types import SimpleNamespace  # Used to create lightweight objects with attribute-style access\nfrom transformers import (\n    GPT2LMHeadModel, GPT2TokenizerFast, get_linear_schedule_with_warmup\n)  # Transformer models, tokenization, and learning rate scheduler\nimport albumentations as A  # Library for augmenting image data\nfrom albumentations.pytorch import ToTensorV2  # Converts images to PyTorch tensors\nfrom PIL import Image  # Library for image processing\nfrom pathlib import Path  # For working with file paths\nfrom sklearn.model_selection import train_test_split  # Utility to split data into train/validation sets\nfrom torch.cuda.amp import GradScaler, autocast  # For mixed precision training\nfrom tqdm.auto import tqdm  # Progress bar for loops\nimport gc  # Garbage collection\nimport json  # To handle JSON files\n\n# Disable parallelism in tokenizers for efficiency\n%env TOKENIZERS_PARALLELISM = false\n\n# Define image transformations for data augmentation\nsample_tfms = [\n    A.HorizontalFlip(),  # Randomly flip images horizontally\n    A.RandomBrightnessContrast(),  # Adjust brightness and contrast\n    A.ColorJitter(),  # Randomly change brightness, contrast, and saturation\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),  # Shift, scale, and rotate\n    A.HueSaturationValue(p=0.3),  # Randomly change hue, saturation, and value\n]\n\n# Transformations for training images\ntrain_tfms = A.Compose([\n    *sample_tfms,  # Include augmentation transformations\n    A.Resize(224, 224),  # Resize images to 224x224\n    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),  # Normalize pixel values\n    ToTensorV2(),  # Convert to PyTorch tensors\n])\n\n# Transformations for validation images\nvalid_tfms = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n    ToTensorV2(),\n])\n\n# Load a pre-trained GPT-2 tokenizer\ntokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token\n# Define a custom dataset class for loading and processing data\nclass Dataset:\n    def __init__(self, df, tfms):\n        \"\"\"\n        Args:\n        - df: DataFrame containing image paths and captions\n        - tfms: Transformations to be applied to images\n        \"\"\"\n        self.df = df\n        self.tfms = tfms\n    def __len__(self):\n        return len(self.df)  # Number of samples in the dataset\n    def __getitem__(self,idx):\n        \"\"\"\n        Load and preprocess a single data sample.\n        Args:\n        - idx: Index of the data sample\n        Returns:\n        - image: Transformed image tensor\n        - input_ids: Tokenized input IDs for the caption\n        - labels: Labels for language modeling\n        \"\"\"\n        sample = self.df.iloc[idx,:]\n        image = sample['image']\n        caption = sample['caption']\n        # Open and process the image\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        augs = self.tfms(image=image)\n        image = augs['image']\n        # Process caption\n        caption = f\"{caption}<|endoftext|>\"\n        input_ids = tokenizer(\n            caption,\n            truncation=True)['input_ids']\n        labels = input_ids.copy()\n        labels[:-1] = input_ids[1:] # Shift labels for language modeling\n\n        return image,input_ids,labels\n\n# COCO 2017\nbase_path = Path('/kaggle/input/coco-2017-dataset/coco2017')\nannot = base_path / 'annotations' / 'captions_train2017.json'\nwith open(annot, 'r') as f:\n    data = json.load(f)['annotations']\n\n# Prepare a DataFrame of image paths and captions\nsamples = [[f'{sample[\"image_id\"]:012d}.jpg', sample['caption']] for sample in data]\ndf = pd.DataFrame(samples, columns=['image', 'caption'])\ndf['image'] = df['image'].apply(lambda x: base_path / 'train2017' / x)\ndf = df.sample(150_000).reset_index(drop=True)  # Sample and reset index\n\n# Visualize some samples from the dataset\nsampled_df = df.sample(n=20)\nfig, axs = plt.subplots(10, 2, figsize=(20, 30))\nfor i, row in enumerate(sampled_df.iterrows()):\n    ax = axs[i // 2, i % 2]\n    image_path = row[1]['image']\n    caption = row[1]['caption']\n    image = Image.open(image_path)\n    ax.imshow(image)\n    ax.axis('off')\n    ax.set_title(caption)\nplt.tight_layout()\nplt.show()\n\n# flickr30k\n\"\"\"\nbase_path = Path('/kaggle/input/flickr30k/flickr30k_images')\ndf = pd.read_csv('/kaggle/input/flickr30k/captions.txt',delimiter=',')\ndf.rename({'image_name':'image','comment': 'caption'},inplace=True,axis=1)\ndf['image'] = df['image'].map(lambda x:base_path / x.strip())\ndf['caption'] = df['caption'].map(lambda x:x.strip().lower())\ndf.head()\n\"\"\"\n# Split the dataset into training and validation sets\ntrain_df, val_df = train_test_split(df,test_size=0.1)\ntrain_df.reset_index(drop=True,inplace=True)\nval_df.reset_index(drop=True,inplace=True)\nprint(len(train_df),len(val_df))\n\n# Create Dataset objects for training and validation\ntrain_ds = Dataset(train_df,train_tfms)\nval_ds = Dataset(val_df,valid_tfms)\n\n#Pad according to the longest sequence in the batch\ndef collate_fn(batch):\n    \"\"\"\n    Custom function to collate and pad batch samples.\n    Args:\n    - batch: List of samples\n    Returns:\n    - image: Batch of images\n    - input_ids: Padded input IDs\n    - labels: Padded labels\n    \"\"\"\n    image = [i[0] for i in batch]\n    input_ids = [i[1] for i in batch]\n    labels = [i[2] for i in batch]\n    # Stack and pad tensors\n    image = torch.stack(image,dim=0)\n    input_ids = tokenizer.pad({'input_ids': input_ids}, padding='longest', return_tensors='pt')['input_ids']\n    labels = tokenizer.pad({'input_ids': labels}, padding='longest', return_tensors='pt')['input_ids']\n    # Mask padding tokens in labels\n    mask = (input_ids!=tokenizer.pad_token_id).long()\n    labels[mask==0]=-100\n    return image, input_ids, labels\n\n# DataLoader for training\ndl = torch.utils.data.DataLoader(train_ds,shuffle=True,batch_size=2,collate_fn=collate_fn)\n_,c,l = next(iter(dl))\nprint(c[0])\nprint(l[0])\n\n# This class implements self-attention, a critical component of the Transformer architecture. \n# It allows a model to attend to different parts of the input sequence to understand context and relationships\nclass GPT2Attention(nn.Module):\n    def __init__(self,config):\n        \"\"\"\n        Initializes the GPT2Attention module.\n\n        Args:\n            config: Configuration object containing:\n                - embed_dim (int): Embedding dimension of the model.\n                - num_heads (int): Number of attention heads.\n                - seq_len (int): Sequence length for positional encoding.\n                - attention_dropout (float): Dropout rate for attention weights.\n                - residual_dropout (float): Dropout rate for the residual connections.\n\n        Attributes:\n            embed_dim: Total embedding dimension.\n            n_heads: Number of attention heads.\n            head_size: Dimension of each head (embed_dim / n_heads).\n            seq_len: Maximum sequence length.\n            c_attn: Linear layer to project input embeddings into query, key, and value vectors.\n            scale: Scaling factor for attention scores (1 / sqrt(head_size)).\n            mask: Lower triangular mask to enforce causal attention.\n            c_proj: Linear layer for projecting output back to embedding dimension.\n            attn_dropout: Dropout applied to attention weights.\n            resid_dropout: Dropout applied to residual connections.\n        \"\"\"\n        # Initializes the parent nn.Module class to ensure proper functionality of PyTorch's module framework (e.g., parameter registration and hooks).\n        super().__init__()\n        # Stores the embedding dimension of the model from the configuration. This is the size of the input feature vector that will be processed by the attention mechanism.\n        self.embed_dim = config.embed_dim\n        # Stores the number of attention heads from the configuration. Multi-head attention splits the input into multiple heads for parallel processing, \n        # improving the model's ability to capture diverse relationships\n        self.n_heads = config.num_heads\n        # Ensures that the embedding dimension is evenly divisible by the number of attention heads. Each attention head will process a subset of the embedding dimension (head_size). \n        # This assertion prevents dimension mismatch issues\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        # Calculates the size of each attention head\n        self.head_size = self.embed_dim // self.n_heads\n        # Stores the sequence length of the input from the configuration. Required for constructing the attention mask to prevent attending to future tokens in the sequence.\n        self.seq_len = config.seq_len\n        # Defines a linear layer to project the input into three separate spaces (queries, keys, and values). The output dimension is 3 × (head_size × n_heads) to simultaneously compute \n        # queries, keys, and values for the attention mechanism.\n        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n        # Precomputes the scaling factor for the dot-product attention mechanism. Scaling the dot product of queries and keys by the inverse square root \n        # of the head size stabilizes the softmax output and improves training.\n        self.scale = self.head_size ** -0.5\n        # Registers a lower triangular mask as a persistent buffer. Prevents attending to future tokens in the sequence during training by masking out \n        # upper triangular elements (causal attention). This ensures autoregressive behavior\n        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n        \n        # Defines a linear layer to project the concatenated output of attention heads back to the original embedding space. Combines the information from all attention heads \n        # into a single vector of the same dimension as the input\n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        # Adds a dropout layer for the attention scores. Prevents overfitting during training by randomly dropping some connections during attention computation.\n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        # Adds a dropout layer for the final output of the attention block. Reduces overfitting by regularizing the residual connection output.\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass for self-attention.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n\n        Returns:\n            out (torch.Tensor): Output tensor of shape (batch_size, seq_len, embed_dim).\n        \"\"\"\n        b,t,c = x.shape\n        # q,k,v shape individually: batch_size x seq_len x embed_dim\n        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n        # Project input to query, key, and value tensors\n        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n        # Reshape for multi-head attention\n        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        # Compute scaled dot-product attention\n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = qk_t.masked_fill(self.mask[:,:,:t,:t]==0,float('-inf'))\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        # Compute attention output\n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        # Project output and apply dropout\n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out\n\nclass GPT2CrossAttention(nn.Module):\n    def __init__(self,config):\n        \"\"\"\n        Initializes the GPT2CrossAttention module.\n\n        Args:\n            config: Configuration object containing:\n                - embed_dim (int): Embedding dimension of the model.\n                - num_heads (int): Number of attention heads.\n                - seq_len (int): Sequence length for positional encoding.\n                - attention_dropout (float): Dropout rate for attention weights.\n                - residual_dropout (float): Dropout rate for the residual connections.\n\n        Attributes:\n            q, k, v: Linear layers for generating query, key, and value vectors.\n            scale: Scaling factor for attention scores (1 / sqrt(head_size)).\n            c_proj: Linear layer for projecting output back to embedding dimension.\n            attn_dropout: Dropout applied to attention weights.\n            resid_dropout: Dropout applied to residual connections.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n        \n        self.q = nn.Linear(self.embed_dim,self.embed_dim)\n        self.k = nn.Linear(self.embed_dim,self.embed_dim)\n        self.v = nn.Linear(self.embed_dim,self.embed_dim)\n\n        self.scale = self.head_size ** -0.5\n        \n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        self.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        \"\"\"\n        Initializes weights for linear layers.\n        \"\"\"\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        \n        \n    def forward(self, q,k,v):\n        \"\"\"\n        Forward pass for cross-attention.\n\n        Args:\n            q (torch.Tensor): Query tensor of shape (batch_size, query_seq_len, embed_dim).\n            k (torch.Tensor): Key tensor of shape (batch_size, key_seq_len, embed_dim).\n            v (torch.Tensor): Value tensor of shape (batch_size, value_seq_len, embed_dim).\n\n        Returns:\n            out (torch.Tensor): Output tensor of shape (batch_size, query_seq_len, embed_dim).\n        \"\"\"\n        b,t,c = q.shape\n        \n        # Project query, key, and value\n        q = self.q(q)\n        k = self.k(k)\n        v = self.v(v)\n        \n        # Reshape for multi-head attention\n        q = q.view(b,q.size(1),self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,k.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,v.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        # Compute scaled dot-product attention\n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        # Compute attention output\n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        # Project output and apply dropout\n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out\n\nclass GPT2MLP(nn.Module):\n    \"\"\"\n    Implements a Multi-Layer Perceptron (MLP) used in the GPT-2 model for feed-forward transformations.\n    The MLP consists of two linear layers with a GELU activation in between, followed by dropout for regularization.\n    It expands the feature dimensions in the hidden layer and projects back to the original size.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # Embedding dimension of the input features\n        self.embed_dim = config.embed_dim\n        # Ratio to expand the input dimension in the hidden layer\n        self.mlp_ratio = config.mlp_ratio\n        # Dropout rate for regularization in the MLP\n        self.mlp_dropout = config.mlp_dropout\n\n        # Linear layer to project input to a larger dimension (embed_dim * mlp_ratio)\n        self.c_fc = nn.Linear(self.embed_dim, self.embed_dim * self.mlp_ratio)\n        # Linear layer to project the expanded dimension back to the original embedding size\n        self.c_proj = nn.Linear(self.embed_dim * self.mlp_ratio, self.embed_dim)\n        # GELU activation function for smooth non-linearity\n        self.act = nn.GELU()\n        # Dropout layer to randomly zero out activations for regularization\n        self.dropout = nn.Dropout(self.mlp_dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the MLP block.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n\n        Returns:\n            Tensor: Output tensor of the same shape as input (batch_size, seq_len, embed_dim).\n        \"\"\"\n        # First linear transformation to expand the input features\n        x = self.c_fc(x)\n        # Apply the GELU activation function\n        x = self.act(x)\n        # Project the expanded features back to the original embedding size\n        x = self.c_proj(x)\n        # Apply dropout for regularization\n        x = self.dropout(x)\n        # Return the transformed features\n        return x\n\nclass GPT2Block(nn.Module):\n    \"\"\"\n    Implements a single block of the GPT-2 architecture, consisting of:\n    - A self-attention mechanism\n    - Cross-attention for encoder-decoder interaction\n    - A feed-forward MLP\n    - Layer normalization after each sub-layer\n    Each sub-layer is followed by residual connections.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # Embedding dimension of the input features\n        self.embed_dim = config.embed_dim\n\n        # Layer normalization applied before the self-attention mechanism\n        self.ln_1 = nn.LayerNorm(self.embed_dim)\n        # Self-attention mechanism\n        self.attn = GPT2Attention(config)\n\n        # Layer normalization applied before the cross-attention mechanism\n        self.ln_2 = nn.LayerNorm(self.embed_dim)\n        # Cross-attention mechanism for encoder-decoder interaction\n        self.cross_attn = GPT2CrossAttention(config)\n\n        # Layer normalization applied before the feed-forward MLP\n        self.ln_3 = nn.LayerNorm(self.embed_dim)\n        # Feed-forward MLP block\n        self.mlp = GPT2MLP(config)\n\n    def forward(self, x, enc_out):\n        \"\"\"\n        Forward pass through the GPT-2 block.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            enc_out (Tensor): Encoder output tensor for cross-attention, shape (batch_size, seq_len, embed_dim).\n\n        Returns:\n            Tensor: Output tensor of shape (batch_size, seq_len, embed_dim).\n        \"\"\"\n        # Apply layer normalization and self-attention, followed by a residual connection\n        x = x + self.attn(self.ln_1(x))\n        # Apply layer normalization and cross-attention with encoder output, followed by a residual connection\n        x = x + self.cross_attn(self.ln_2(x), enc_out, enc_out)\n        # Apply layer normalization and the MLP, followed by a residual connection\n        x = x + self.mlp(self.ln_3(x))\n        # Return the final output\n        return x\n\n\nclass VisionGPT2Model(nn.Module):\n    \"\"\"\n    Combines a Vision Transformer (ViT) with GPT-2 to create a vision-language model.\n    The model integrates image patch embeddings from ViT with GPT-2's transformer layers for sequence modeling.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = config\n\n        # Load a pre-trained Vision Transformer (ViT) model with no output classification layer\n        vit = create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n\n        # Patch embedding module from ViT, which divides the input image into patches and embeds them\n        self.patch_embed = vit.patch_embed\n        # Total number of patches generated by the ViT patch embedding\n        num_patches = self.patch_embed.num_patches\n\n        # Class token for ViT, used to aggregate global information from all patches\n        self.cls_token = vit.cls_token\n\n        # Compute the total embedding length, including class token and positional embeddings\n        embed_len = num_patches + vit.num_prefix_tokens\n        # Positional embedding for the patches and class token\n        self.pos_embed = vit.pos_embed\n        # Dropout for positional embeddings to reduce overfitting\n        self.pos_drop = nn.Dropout(p=0.)\n\n        # Subset of ViT transformer blocks up to the specified depth in the configuration\n        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n\n        # GPT-2 style transformer, adapted for vision-language tasks\n        self.transformer = nn.ModuleDict(dict(\n            # Token embeddings for input vocabulary\n            wte=nn.Embedding(config.vocab_size, config.embed_dim),\n            # Positional embeddings for sequence tokens\n            wpe=nn.Embedding(config.seq_len, config.embed_dim),\n            # Dropout for embeddings\n            drop=nn.Dropout(config.emb_dropout),\n            # Transformer blocks for processing sequences\n            h=nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n            # Final layer normalization\n            ln_f=nn.LayerNorm(config.embed_dim)\n        ))\n\n        # Linear layer to project transformer output to vocabulary logits\n        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n        # Share weights between the embedding layer and the language modeling head\n        self.transformer.wte.weight = self.lm_head.weight\n\n    def _pos_embed(self, x):\n        \"\"\"\n        Add positional embeddings to input patches and prepend the class token.\n\n        Args:\n            x (Tensor): Patch embeddings of shape (batch_size, num_patches, embed_dim).\n\n        Returns:\n            Tensor: Input with positional embeddings and class token, shape (batch_size, embed_len, embed_dim).\n        \"\"\"\n        # Retrieve positional embeddings\n        pos_embed = self.pos_embed\n        # Prepend class token to patch embeddings\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        # Add positional embeddings\n        x = x + pos_embed\n        # Apply dropout\n        return self.pos_drop(x)\n\n    def pretrained_layers_trainable(self, trainable=False):\n        \"\"\"\n        Toggle trainability of pretrained layers (ViT and GPT-2).\n\n        Args:\n            trainable (bool): Whether to make layers trainable (True) or freeze them (False).\n        \"\"\"\n        # List of all layers in the model\n        layers = [\n            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n            self.transformer.wte, self.transformer.wpe,\n            self.transformer.ln_f, self.lm_head\n        ]\n\n        # Add GPT-2 transformer layers to the list\n        gpt_layers = [[\n            self.transformer.h[i].ln_1, self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn, self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        for l in gpt_layers:\n            layers.extend(l)\n\n        # Set the requires_grad property for each layer's parameters\n        for layer in layers:\n            if not isinstance(layer, nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = trainable\n            else:\n                layer.requires_grad = trainable\n\n        # Calculate and print the number of frozen parameters\n        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n        print(f'{total_frozen_params=}')\n\n    def unfreeze_gpt_layers(self):\n        \"\"\"\n        Unfreeze all GPT-2 transformer layers, enabling them to be trainable.\n        \"\"\"\n        # Flatten all GPT-2 layers for easy iteration\n        gpt_layers = [[\n            self.transformer.h[i].ln_1, self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn, self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        flatten = []\n        for l in gpt_layers:\n            flatten.extend(l)\n\n        # Set the requires_grad property for each parameter in GPT-2 layers\n        for layer in flatten:\n            if not isinstance(layer, nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = True\n            else:\n                layer.requires_grad = True\n\n    # The @classmethod decorator in Python is used to define a method that belongs to the class itself rather than to a specific instance of the class. \n    # This means the method can be called on the class directly without needing to instantiate an object of the class.  \n    @classmethod\n    def from_pretrained(cls, config):\n        \"\"\"\n        Load a pre-trained VisionGPT2Model by combining ViT and GPT-2 pretrained weights.\n\n        Args:\n            config (object): Configuration object containing model parameters.\n\n        Returns:\n            VisionGPT2Model: A VisionGPT2Model instance initialized with pretrained weights.\n        \"\"\"\n        # Initialize a new model instance\n        model = cls(config)\n        sd = model.state_dict()  # Model's current state dictionary\n        keys = sd.keys()  # All keys in the state dictionary\n\n        # Define patterns to identify which weights to ignore\n        ignore_matches = ['blocks.', 'cross_attn.', 'ln_3', 'cls_token', 'pos_embed', 'patch_embed.', '.attn.mask']\n\n        # Separate ViT-specific keys\n        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n        # Remaining keys are considered GPT-specific\n        gpt_keys = [key for key in keys if key not in vit_keys]\n\n        # Load pretrained GPT-2 weights\n        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n        sd_hf = gpt2_small.state_dict()  # GPT-2's state dictionary\n        hf_keys = sd_hf.keys()  # All GPT-2 keys\n\n        # Filter GPT-2 keys to exclude unnecessary masked bias and bias keys\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n\n        # Define keys that need transposing for weight shape compatibility\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n\n        # Copy relevant weights from GPT-2 to the model\n        for k in hf_keys:\n            if any(match in k for match in ignore_matches):\n                continue  # Skip ignored keys\n            if any(k.endswith(w) for w in transposed):  # Transpose weights if needed\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:  # Direct copy for matching shapes\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        # Load updated state dictionary into the model\n        model.load_state_dict(sd)\n\n        return model\n\n    def forward(self, image, input_ids, labels=None):\n        \"\"\"\n        Forward pass for vision-language processing.\n\n        Args:\n            image (Tensor): Input image tensor.\n            input_ids (Tensor): Sequence of token IDs.\n            labels (Tensor, optional): Ground truth labels for loss computation.\n\n        Returns:\n            Tensor: Loss value if labels are provided, else logits of the final token.\n        \"\"\"\n        # Extract patch embeddings from the input image\n        image = self.patch_embed(image)\n        image = self._pos_embed(image)\n\n        # Get token embeddings and positional embeddings\n        token_embeddings = self.transformer.wte(input_ids)  # Token embeddings\n        pos_embs = torch.arange(0, input_ids.size(1)).to(input_ids.device)\n        positional_embeddings = self.transformer.wpe(pos_embs)  # Positional embeddings\n        input_ids = self.transformer.drop(token_embeddings + positional_embeddings)  # Combine with dropout\n\n        # Process both image and input tokens through transformer layers\n        for i in range(self.config.depth):\n            image = self.blocks[i](image)  # Process image embeddings through ViT blocks\n            input_ids = self.transformer.h[i](input_ids, image)  # Fuse image and token embeddings\n\n        # Apply final layer normalization\n        input_ids = self.transformer.ln_f(input_ids)\n\n        # Compute loss if labels are provided\n        if labels is not None:\n            lm_logits = self.lm_head(input_ids)  # Project to vocabulary space\n            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n            return loss\n\n        # Return logits of the last token if no labels are provided\n        lm_logits = self.lm_head(input_ids[:, [-1], :])\n        return lm_logits\n\n    def generate(self, image, sequence, max_tokens=50, temperature=1.0, deterministic=False):\n        \"\"\"\n        Generate a sequence of tokens using autoregressive decoding.\n\n        Args:\n            image (Tensor): Input image tensor.\n            sequence (Tensor): Initial sequence of token IDs.\n            max_tokens (int): Maximum number of tokens to generate.\n            temperature (float): Sampling temperature to control randomness.\n            deterministic (bool): Use deterministic (greedy) sampling if True.\n\n        Returns:\n            Tensor: Generated token sequence.\n        \"\"\"\n        for _ in range(max_tokens):\n            # Predict the next token\n            out = self(image, sequence)\n            out = out[:, -1, :] / temperature  # Normalize logits by temperature\n            probs = F.softmax(out, dim=-1)  # Convert logits to probabilities\n\n            # Select the next token deterministically or probabilistically\n            if deterministic:\n                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n            else:\n                next_token = torch.multinomial(probs, num_samples=1)\n\n            # Append the predicted token to the sequence\n            sequence = torch.cat([sequence, next_token], dim=1)\n\n            # Stop generation if the end-of-sequence token is generated\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n        return sequence.cpu().flatten()\n\nclass Trainer:\n    def __init__(self, model_config, train_config, dls):\n        \"\"\"\n        Initialize the Trainer with configurations and data loaders.\n\n        Args:\n            model_config (SimpleNamespace): Configuration for the VisionGPT2Model.\n            train_config (SimpleNamespace): Training configurations, including epochs, learning rate, etc.\n            dls (tuple): A tuple containing training and validation dataloaders.\n\n        Attributes:\n            model: The VisionGPT2Model initialized with pretrained weights.\n            tokenizer: GPT2 tokenizer for text processing.\n            scaler: Gradient scaler for mixed-precision training.\n            train_dl, val_dl: Training and validation dataloaders.\n            optim: Adam optimizer for the model.\n            sched: OneCycleLR scheduler for learning rate scheduling.\n            metrics: DataFrame to track training and validation losses and perplexities.\n            gen_tfms: Transformations for image preprocessing during caption generation.\n        \"\"\"\n        self.train_config = train_config\n        self.model_config = model_config\n        self.device = self.train_config.device\n\n        # Load and prepare the model\n        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n        self.model.pretrained_layers_trainable(trainable=False)\n\n        print(f'Trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n\n        # Prepare tokenizer\n        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        # Gradient scaler for mixed precision\n        self.scaler = GradScaler()\n\n        # Data loaders\n        self.train_dl, self.val_dl = dls\n\n        # Optimizer and scheduler\n        total_steps = len(self.train_dl)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.0)\n        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n            self.optim,\n            max_lr=self.train_config.lr,\n            epochs=self.train_config.epochs,\n            steps_per_epoch=total_steps\n        )\n\n        # Metrics DataFrame\n        self.metrics = pd.DataFrame()\n        self.metrics[['train_loss', 'train_perplexity', 'val_loss', 'val_perplexity']] = None\n\n        # Image preprocessing for caption generation\n        self.gen_tfms = A.Compose([\n            A.Resize(224, 224),\n            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n            ToTensorV2()\n        ])\n\n    def save_model(self):\n        \"\"\"Save the current model state to a file.\"\"\"\n        self.train_config.model_path.mkdir(exist_ok=True)\n        sd = self.model.state_dict()\n        torch.save(sd, self.train_config.model_path / 'captioner.pt')\n\n    def load_best_model(self):\n        \"\"\"Load the best saved model from file.\"\"\"\n        sd = torch.load(self.train_config.model_path / 'captioner.pt')\n        self.model.load_state_dict(sd)\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Train the model for one epoch.\n\n        Args:\n            epoch (int): Current epoch number.\n        \"\"\"\n        prog = tqdm(self.train_dl, total=len(self.train_dl))\n        running_loss = 0.0\n\n        for image, input_ids, labels in prog:\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image, input_ids, labels)\n\n                # Backpropagation and optimization\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optim)\n                self.scaler.update()\n                self.sched.step()\n                self.optim.zero_grad(set_to_none=True)\n\n                running_loss += loss.item()\n                prog.set_description(f'Train loss: {loss.item():.3f}')\n\n            # Clean up to save memory\n            del image, input_ids, labels, loss\n\n        # Compute average loss and perplexity\n        train_loss = running_loss / len(self.train_dl)\n        train_pxp = np.exp(train_loss)\n        self.metrics.loc[epoch, ['train_loss', 'train_perplexity']] = (train_loss, train_pxp)\n\n    @torch.no_grad()\n    def valid_one_epoch(self, epoch):\n        \"\"\"\n        Validate the model for one epoch.\n\n        Args:\n            epoch (int): Current epoch number.\n\n        Returns:\n            float: Validation perplexity.\n        \"\"\"\n        prog = tqdm(self.val_dl, total=len(self.val_dl))\n        running_loss = 0.0\n\n        for image, input_ids, labels in prog:\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image, input_ids, labels)\n                running_loss += loss.item()\n                prog.set_description(f'Valid loss: {loss.item():.3f}')\n\n            # Clean up to save memory\n            del image, input_ids, labels, loss\n\n        # Compute average loss and perplexity\n        val_loss = running_loss / len(self.val_dl)\n        val_pxp = np.exp(val_loss)\n        self.metrics.loc[epoch, ['val_loss', 'val_perplexity']] = (val_loss, val_pxp)\n        return val_pxp\n\n    def clean(self):\n        \"\"\"Perform garbage collection and free CUDA memory.\"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def fit(self):\n        \"\"\"\n        Train and validate the model for multiple epochs.\n\n        Returns:\n            dict: Best perplexity and corresponding epoch.\n        \"\"\"\n        best_pxp = 1e9\n        best_epoch = -1\n        prog = tqdm(range(self.train_config.epochs))\n\n        for epoch in prog:\n            if epoch == self.train_config.freeze_epochs_gpt:\n                self.model.unfreeze_gpt_layers()\n                print('Unfreezing GPT2 entirely...')\n\n            if epoch == self.train_config.freeze_epochs_all:\n                self.model.pretrained_layers_trainable(trainable=True)\n\n            # Training phase\n            self.model.train()\n            prog.set_description('Training')\n            self.train_one_epoch(epoch)\n            self.clean()\n\n            # Validation phase\n            self.model.eval()\n            prog.set_description('Validating')\n            pxp = self.valid_one_epoch(epoch)\n            self.clean()\n\n            print(self.metrics.tail(1))\n\n            # Save the best model\n            if pxp < best_pxp:\n                best_pxp = pxp\n                best_epoch = epoch\n                print('Saving best model...')\n                self.save_model()\n\n        return {\n            'best_perplexity': best_pxp,\n            'best_epoch': best_epoch\n        }\n\n    @torch.no_grad()\n    def generate_caption(self, image, max_tokens=50, temperature=1.0, deterministic=False):\n        \"\"\"\n        Generate a caption for the given image.\n\n        Args:\n            image (str): Path to the input image.\n            max_tokens (int): Maximum number of tokens to generate.\n            temperature (float): Sampling temperature.\n            deterministic (bool): If True, use deterministic generation.\n\n        Returns:\n            str: Generated caption.\n        \"\"\"\n        self.model.eval()\n\n        # Preprocess the image\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        image = self.gen_tfms(image=image)['image']\n        image = image.unsqueeze(0).to(self.device)\n\n        # Prepare initial sequence\n        sequence = torch.ones(1, 1).to(device=self.device).long() * self.tokenizer.bos_token_id\n\n        # Generate caption\n        caption = self.model.generate(\n            image,\n            sequence,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            deterministic=deterministic\n        )\n        caption = self.tokenizer.decode(caption.numpy(), skip_special_tokens=True)\n\n        return caption\n\n\n# Define model configuration using SimpleNamespace for easy attribute access\nmodel_config = SimpleNamespace(\n    vocab_size=50_257,            # Size of the vocabulary (e.g., GPT-2's vocabulary size)\n    embed_dim=768,                # Embedding dimension for each token\n    num_heads=12,                 # Number of attention heads in each transformer block\n    seq_len=1024,                 # Maximum sequence length supported by the model\n    depth=12,                     # Number of transformer layers in the model\n    attention_dropout=0.1,        # Dropout rate for the attention mechanism\n    residual_dropout=0.1,         # Dropout rate for residual connections\n    mlp_ratio=4,                  # Ratio for the feed-forward layer's hidden size to input size\n    mlp_dropout=0.1,              # Dropout rate in the feed-forward layer\n    emb_dropout=0.1               # Dropout rate for token embeddings\n)\n\n# Define training configuration using SimpleNamespace\ntrain_config = SimpleNamespace(\n    epochs=5,                     # Total number of training epochs\n    freeze_epochs_gpt=1,          # Number of epochs to freeze GPT layers during training\n    freeze_epochs_all=2,          # Number of epochs to freeze all pretrained layers\n    lr=1e-4,                      # Initial learning rate for training\n    device='cuda',                # Device to use for training ('cuda' for GPU or 'cpu' for CPU)\n    model_path=Path('captioner'), # Path to save the trained model\n    batch_size=32                 # Batch size for training and validation\n)\n\n# Create DataLoader for the training dataset\ntrain_dl = torch.utils.data.DataLoader(\n    train_ds,                      # Training dataset\n    batch_size=train_config.batch_size, # Batch size for training\n    shuffle=True,                  # Shuffle data at every epoch\n    pin_memory=True,               # Pin memory for faster data transfer to GPU\n    num_workers=2,                 # Number of workers for data loading\n    persistent_workers=True,       # Keep workers alive across epochs for efficiency\n    collate_fn=collate_fn          # Custom function to process and batch data\n)\n\n# Create DataLoader for the validation dataset\nval_dl = torch.utils.data.DataLoader(\n    val_ds,                        # Validation dataset\n    batch_size=train_config.batch_size, # Batch size for validation\n    shuffle=False,                 # Do not shuffle validation data\n    pin_memory=True,               # Pin memory for faster data transfer to GPU\n    num_workers=2,                 # Number of workers for data loading\n    persistent_workers=True,       # Keep workers alive across epochs for efficiency\n    collate_fn=collate_fn          # Custom function to process and batch data\n)\n\n# Initialize the Trainer class with model configuration, training configuration, and DataLoaders\ntrainer = Trainer(model_config, train_config, (train_dl, val_dl))\n\n# Train the model and fit it on the dataset\ntrainer.fit()\n\n# View the metrics collected during training and validation\ntrainer.metrics\n\n# Plot training and validation loss\nplt.plot(trainer.metrics['train_loss'], color='red', label='train loss')  # Plot train loss\nplt.plot(trainer.metrics['val_loss'], color='orange', label='valid loss')  # Plot validation loss\nplt.title('Loss, lower=better')  # Title for the plot\nplt.legend()  # Add legend to differentiate curves\nplt.show()  # Display the plot\n\n# Plot training and validation perplexity\nplt.plot(trainer.metrics['train_perplexity'], color='blue', label='train perplexity')  # Train perplexity\nplt.plot(trainer.metrics['val_perplexity'], color='lightblue', label='valid perplexity')  # Validation perplexity\nplt.title('Perplexity, lower=better')  # Title for the plot\nplt.legend()  # Add legend to differentiate curves\nplt.show()  # Display the plot\n\n# Load the best model based on validation performance\ntrainer.load_best_model()\n\n# Test the model by generating captions for random samples from the validation dataset\nfor i in range(50):  # Generate captions for 50 samples\n    det = False  # Deterministic generation flag, initially set to False\n    test = val_df.sample(n=1).values[0]  # Randomly sample one validation data point\n    test_img, test_caption = test[0], test[1]  # Extract image path and actual caption\n\n    # Display the test image\n    plt.imshow(Image.open(test_img).convert('RGB'))  # Open and convert the image to RGB format\n\n    # Generate a random temperature for diversity in generation\n    t = np.random.uniform(0.5, 1.5)\n\n    # Use deterministic generation for the last 10 samples\n    if i > 40:\n        det = True\n\n    # Generate a caption for the test image\n    gen_caption = trainer.generate_caption(test_img, temperature=t, deterministic=det)\n\n    # Display the actual and generated captions along with generation parameters\n    plt.title(f\"actual: {test_caption}\\nmodel: {gen_caption}\\ntemp: {t} deterministic generation: {det}\")\n    plt.axis('off')  # Remove axis for better visual appeal\n    plt.show()  # Show the image and captions","metadata":{"_uuid":"dceb37ef-f954-4785-8aa4-742c5f2483b8","_cell_guid":"6d78aba9-7b7f-493f-8584-eb7bb624260d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are deficiencies in version1:\n1. fixed learning rate of 1e-4; Implement learning rate scheduling\n2. Add gradient clipping after the backward pass, inside the training loop\n3. Implement image augmentation techniques such as random rotations, flips, scaling, brightness adjustments, and color jittering\n4. Use ResNet or maybe Vision Transformer for feature extraction","metadata":{}},{"cell_type":"code","source":"# Importing required libraries and modules\nimport torch  # Core library for deep learning\nimport torch.nn as nn  # Provides building blocks for neural networks\nimport torch.nn.functional as F  # Commonly used functions like activation functions\nimport numpy as np  # Library for numerical operations\nimport pandas as pd  # For handling data in tabular format\nimport matplotlib.pyplot as plt  # For visualizing data\nfrom timm import create_model, list_models  # Pretrained models and utilities\nfrom types import SimpleNamespace  # Used to create lightweight objects with attribute-style access\nfrom transformers import (\n    GPT2LMHeadModel, GPT2TokenizerFast, get_linear_schedule_with_warmup\n)  # Transformer models, tokenization, and learning rate scheduler\nimport albumentations as A  # Library for augmenting image data\nfrom albumentations.pytorch import ToTensorV2  # Converts images to PyTorch tensors\nfrom PIL import Image  # Library for image processing\nfrom pathlib import Path  # For working with file paths\nfrom sklearn.model_selection import train_test_split  # Utility to split data into train/validation sets\nfrom torch.cuda.amp import GradScaler, autocast  # For mixed precision training\nfrom tqdm.auto import tqdm  # Progress bar for loops\nimport gc  # Garbage collection\nimport json  # To handle JSON files\n\n# Disable parallelism in tokenizers for efficiency\n%env TOKENIZERS_PARALLELISM = false\n\n# Define image transformations for data augmentation\nsample_tfms = [\n    A.HorizontalFlip(),  # Randomly flip images horizontally\n    A.RandomBrightnessContrast(),  # Adjust brightness and contrast\n    A.ColorJitter(),  # Randomly change brightness, contrast, and saturation\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),  # Shift, scale, and rotate\n    A.HueSaturationValue(p=0.3),  # Randomly change hue, saturation, and value\n]\n\n# Transformations for training images\ntrain_tfms = A.Compose([\n    A.RandomResizedCrop(224, 224, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n    A.ColorJitter(hue=0.05, saturation=0.05, p=0.5),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n    *sample_tfms,  # Include augmentation transformations\n    A.Resize(224, 224),  # Resize images to 224x224\n    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),  # Normalize pixel values\n    ToTensorV2(),  # Convert to PyTorch tensors\n])\n\n# Transformations for validation images\nvalid_tfms = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n    ToTensorV2(),\n])\n\n# Load a pre-trained GPT-2 tokenizer\ntokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token\n# Define a custom dataset class for loading and processing data\nclass Dataset:\n    def __init__(self, df, tfms):\n        \"\"\"\n        Args:\n        - df: DataFrame containing image paths and captions\n        - tfms: Transformations to be applied to images\n        \"\"\"\n        self.df = df\n        self.tfms = tfms\n    def __len__(self):\n        return len(self.df)  # Number of samples in the dataset\n    def __getitem__(self,idx):\n        \"\"\"\n        Load and preprocess a single data sample.\n        Args:\n        - idx: Index of the data sample\n        Returns:\n        - image: Transformed image tensor\n        - input_ids: Tokenized input IDs for the caption\n        - labels: Labels for language modeling\n        \"\"\"\n        sample = self.df.iloc[idx,:]\n        image = sample['image']\n        caption = sample['caption']\n        # Open and process the image\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        augs = self.tfms(image=image)\n        image = augs['image']\n        # Process caption\n        caption = f\"{caption}<|endoftext|>\"\n        input_ids = tokenizer(\n            caption,\n            truncation=True)['input_ids']\n        labels = input_ids.copy()\n        labels[:-1] = input_ids[1:] # Shift labels for language modeling\n\n        return image,input_ids,labels\n\n# COCO 2017\nbase_path = Path('/kaggle/input/coco-2017-dataset/coco2017')\nannot = base_path / 'annotations' / 'captions_train2017.json'\nwith open(annot, 'r') as f:\n    data = json.load(f)['annotations']\n\n# Prepare a DataFrame of image paths and captions\nsamples = [[f'{sample[\"image_id\"]:012d}.jpg', sample['caption']] for sample in data]\ndf = pd.DataFrame(samples, columns=['image', 'caption'])\ndf['image'] = df['image'].apply(lambda x: base_path / 'train2017' / x)\ndf = df.sample(150_000).reset_index(drop=True)  # Sample and reset index\n\n# Visualize some samples from the dataset\nsampled_df = df.sample(n=20)\nfig, axs = plt.subplots(10, 2, figsize=(20, 30))\nfor i, row in enumerate(sampled_df.iterrows()):\n    ax = axs[i // 2, i % 2]\n    image_path = row[1]['image']\n    caption = row[1]['caption']\n    image = Image.open(image_path)\n    ax.imshow(image)\n    ax.axis('off')\n    ax.set_title(caption)\nplt.tight_layout()\nplt.show()\n\n# flickr30k\n\"\"\"\nbase_path = Path('/kaggle/input/flickr30k/flickr30k_images')\ndf = pd.read_csv('/kaggle/input/flickr30k/captions.txt',delimiter=',')\ndf.rename({'image_name':'image','comment': 'caption'},inplace=True,axis=1)\ndf['image'] = df['image'].map(lambda x:base_path / x.strip())\ndf['caption'] = df['caption'].map(lambda x:x.strip().lower())\ndf.head()\n\"\"\"\n# Split the dataset into training and validation sets\ntrain_df, val_df = train_test_split(df,test_size=0.1)\ntrain_df.reset_index(drop=True,inplace=True)\nval_df.reset_index(drop=True,inplace=True)\nprint(len(train_df),len(val_df))\n\n# Create Dataset objects for training and validation\ntrain_ds = Dataset(train_df,train_tfms)\nval_ds = Dataset(val_df,valid_tfms)\n\n#Pad according to the longest sequence in the batch\ndef collate_fn(batch):\n    \"\"\"\n    Custom function to collate and pad batch samples.\n    Args:\n    - batch: List of samples\n    Returns:\n    - image: Batch of images\n    - input_ids: Padded input IDs\n    - labels: Padded labels\n    \"\"\"\n    image = [i[0] for i in batch]\n    input_ids = [i[1] for i in batch]\n    labels = [i[2] for i in batch]\n    # Stack and pad tensors\n    image = torch.stack(image,dim=0)\n    input_ids = tokenizer.pad({'input_ids': input_ids}, padding='longest', return_tensors='pt')['input_ids']\n    labels = tokenizer.pad({'input_ids': labels}, padding='longest', return_tensors='pt')['input_ids']\n    # Mask padding tokens in labels\n    mask = (input_ids!=tokenizer.pad_token_id).long()\n    labels[mask==0]=-100\n    return image, input_ids, labels\n\n# DataLoader for training\ndl = torch.utils.data.DataLoader(train_ds,shuffle=True,batch_size=2,collate_fn=collate_fn)\n_,c,l = next(iter(dl))\nprint(c[0])\nprint(l[0])\n\n# This class implements self-attention, a critical component of the Transformer architecture. \n# It allows a model to attend to different parts of the input sequence to understand context and relationships\nclass GPT2Attention(nn.Module):\n    def __init__(self,config):\n        \"\"\"\n        Initializes the GPT2Attention module.\n\n        Args:\n            config: Configuration object containing:\n                - embed_dim (int): Embedding dimension of the model.\n                - num_heads (int): Number of attention heads.\n                - seq_len (int): Sequence length for positional encoding.\n                - attention_dropout (float): Dropout rate for attention weights.\n                - residual_dropout (float): Dropout rate for the residual connections.\n\n        Attributes:\n            embed_dim: Total embedding dimension.\n            n_heads: Number of attention heads.\n            head_size: Dimension of each head (embed_dim / n_heads).\n            seq_len: Maximum sequence length.\n            c_attn: Linear layer to project input embeddings into query, key, and value vectors.\n            scale: Scaling factor for attention scores (1 / sqrt(head_size)).\n            mask: Lower triangular mask to enforce causal attention.\n            c_proj: Linear layer for projecting output back to embedding dimension.\n            attn_dropout: Dropout applied to attention weights.\n            resid_dropout: Dropout applied to residual connections.\n        \"\"\"\n        # Initializes the parent nn.Module class to ensure proper functionality of PyTorch's module framework (e.g., parameter registration and hooks).\n        super().__init__()\n        # Stores the embedding dimension of the model from the configuration. This is the size of the input feature vector that will be processed by the attention mechanism.\n        self.embed_dim = config.embed_dim\n        # Stores the number of attention heads from the configuration. Multi-head attention splits the input into multiple heads for parallel processing, \n        # improving the model's ability to capture diverse relationships\n        self.n_heads = config.num_heads\n        # Ensures that the embedding dimension is evenly divisible by the number of attention heads. Each attention head will process a subset of the embedding dimension (head_size). \n        # This assertion prevents dimension mismatch issues\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        # Calculates the size of each attention head\n        self.head_size = self.embed_dim // self.n_heads\n        # Stores the sequence length of the input from the configuration. Required for constructing the attention mask to prevent attending to future tokens in the sequence.\n        self.seq_len = config.seq_len\n        # Defines a linear layer to project the input into three separate spaces (queries, keys, and values). The output dimension is 3 × (head_size × n_heads) to simultaneously compute \n        # queries, keys, and values for the attention mechanism.\n        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n        # Precomputes the scaling factor for the dot-product attention mechanism. Scaling the dot product of queries and keys by the inverse square root \n        # of the head size stabilizes the softmax output and improves training.\n        self.scale = self.head_size ** -0.5\n        # Registers a lower triangular mask as a persistent buffer. Prevents attending to future tokens in the sequence during training by masking out \n        # upper triangular elements (causal attention). This ensures autoregressive behavior\n        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n        \n        # Defines a linear layer to project the concatenated output of attention heads back to the original embedding space. Combines the information from all attention heads \n        # into a single vector of the same dimension as the input\n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        # Adds a dropout layer for the attention scores. Prevents overfitting during training by randomly dropping some connections during attention computation.\n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        # Adds a dropout layer for the final output of the attention block. Reduces overfitting by regularizing the residual connection output.\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass for self-attention.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n\n        Returns:\n            out (torch.Tensor): Output tensor of shape (batch_size, seq_len, embed_dim).\n        \"\"\"\n        b,t,c = x.shape\n        # q,k,v shape individually: batch_size x seq_len x embed_dim\n        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n        # Project input to query, key, and value tensors\n        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n        # Reshape for multi-head attention\n        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        # Compute scaled dot-product attention\n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = qk_t.masked_fill(self.mask[:,:,:t,:t]==0,float('-inf'))\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        # Compute attention output\n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        # Project output and apply dropout\n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out\n\nclass GPT2CrossAttention(nn.Module):\n    def __init__(self,config):\n        \"\"\"\n        Initializes the GPT2CrossAttention module.\n\n        Args:\n            config: Configuration object containing:\n                - embed_dim (int): Embedding dimension of the model.\n                - num_heads (int): Number of attention heads.\n                - seq_len (int): Sequence length for positional encoding.\n                - attention_dropout (float): Dropout rate for attention weights.\n                - residual_dropout (float): Dropout rate for the residual connections.\n\n        Attributes:\n            q, k, v: Linear layers for generating query, key, and value vectors.\n            scale: Scaling factor for attention scores (1 / sqrt(head_size)).\n            c_proj: Linear layer for projecting output back to embedding dimension.\n            attn_dropout: Dropout applied to attention weights.\n            resid_dropout: Dropout applied to residual connections.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n        \n        self.q = nn.Linear(self.embed_dim,self.embed_dim)\n        self.k = nn.Linear(self.embed_dim,self.embed_dim)\n        self.v = nn.Linear(self.embed_dim,self.embed_dim)\n\n        self.scale = self.head_size ** -0.5\n        \n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        \n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n        \n        self.apply(self._init_weights)\n        \n    def _init_weights(self, module):\n        \"\"\"\n        Initializes weights for linear layers.\n        \"\"\"\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        \n        \n    def forward(self, q,k,v):\n        \"\"\"\n        Forward pass for cross-attention.\n\n        Args:\n            q (torch.Tensor): Query tensor of shape (batch_size, query_seq_len, embed_dim).\n            k (torch.Tensor): Key tensor of shape (batch_size, key_seq_len, embed_dim).\n            v (torch.Tensor): Value tensor of shape (batch_size, value_seq_len, embed_dim).\n\n        Returns:\n            out (torch.Tensor): Output tensor of shape (batch_size, query_seq_len, embed_dim).\n        \"\"\"\n        b,t,c = q.shape\n        \n        # Project query, key, and value\n        q = self.q(q)\n        k = self.k(k)\n        v = self.v(v)\n        \n        # Reshape for multi-head attention\n        q = q.view(b,q.size(1),self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,k.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,v.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        \n        # Compute scaled dot-product attention\n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n        \n        # Compute attention output\n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n        \n        # Project output and apply dropout\n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n        \n        return out\n\nclass GPT2MLP(nn.Module):\n    \"\"\"\n    Implements a Multi-Layer Perceptron (MLP) used in the GPT-2 model for feed-forward transformations.\n    The MLP consists of two linear layers with a GELU activation in between, followed by dropout for regularization.\n    It expands the feature dimensions in the hidden layer and projects back to the original size.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # Embedding dimension of the input features\n        self.embed_dim = config.embed_dim\n        # Ratio to expand the input dimension in the hidden layer\n        self.mlp_ratio = config.mlp_ratio\n        # Dropout rate for regularization in the MLP\n        self.mlp_dropout = config.mlp_dropout\n\n        # Linear layer to project input to a larger dimension (embed_dim * mlp_ratio)\n        self.c_fc = nn.Linear(self.embed_dim, self.embed_dim * self.mlp_ratio)\n        # Linear layer to project the expanded dimension back to the original embedding size\n        self.c_proj = nn.Linear(self.embed_dim * self.mlp_ratio, self.embed_dim)\n        # GELU activation function for smooth non-linearity\n        self.act = nn.GELU()\n        # Dropout layer to randomly zero out activations for regularization\n        self.dropout = nn.Dropout(self.mlp_dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the MLP block.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n\n        Returns:\n            Tensor: Output tensor of the same shape as input (batch_size, seq_len, embed_dim).\n        \"\"\"\n        # First linear transformation to expand the input features\n        x = self.c_fc(x)\n        # Apply the GELU activation function\n        x = self.act(x)\n        # Project the expanded features back to the original embedding size\n        x = self.c_proj(x)\n        # Apply dropout for regularization\n        x = self.dropout(x)\n        # Return the transformed features\n        return x\n\nclass GPT2Block(nn.Module):\n    \"\"\"\n    Implements a single block of the GPT-2 architecture, consisting of:\n    - A self-attention mechanism\n    - Cross-attention for encoder-decoder interaction\n    - A feed-forward MLP\n    - Layer normalization after each sub-layer\n    Each sub-layer is followed by residual connections.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        # Embedding dimension of the input features\n        self.embed_dim = config.embed_dim\n\n        # Layer normalization applied before the self-attention mechanism\n        self.ln_1 = nn.LayerNorm(self.embed_dim)\n        # Self-attention mechanism\n        self.attn = GPT2Attention(config)\n\n        # Layer normalization applied before the cross-attention mechanism\n        self.ln_2 = nn.LayerNorm(self.embed_dim)\n        # Cross-attention mechanism for encoder-decoder interaction\n        self.cross_attn = GPT2CrossAttention(config)\n\n        # Layer normalization applied before the feed-forward MLP\n        self.ln_3 = nn.LayerNorm(self.embed_dim)\n        # Feed-forward MLP block\n        self.mlp = GPT2MLP(config)\n\n    def forward(self, x, enc_out):\n        \"\"\"\n        Forward pass through the GPT-2 block.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            enc_out (Tensor): Encoder output tensor for cross-attention, shape (batch_size, seq_len, embed_dim).\n\n        Returns:\n            Tensor: Output tensor of shape (batch_size, seq_len, embed_dim).\n        \"\"\"\n        # Apply layer normalization and self-attention, followed by a residual connection\n        x = x + self.attn(self.ln_1(x))\n        # Apply layer normalization and cross-attention with encoder output, followed by a residual connection\n        x = x + self.cross_attn(self.ln_2(x), enc_out, enc_out)\n        # Apply layer normalization and the MLP, followed by a residual connection\n        x = x + self.mlp(self.ln_3(x))\n        # Return the final output\n        return x\n\n\nclass VisionGPT2Model(nn.Module):\n    \"\"\"\n    Combines a Vision Transformer (ViT) with GPT-2 to create a vision-language model.\n    The model integrates image patch embeddings from ViT with GPT-2's transformer layers for sequence modeling.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = config\n\n        # Load a pre-trained Vision Transformer (ViT) model with no output classification layer\n        from transformers import ViTModel\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n\n        # Patch embedding module from ViT, which divides the input image into patches and embeds them\n        self.patch_embed = None  # Patched, as ViT already provides full feature extraction\n        # Total number of patches generated by the ViT patch embedding\n        num_patches = self.patch_embed.num_patches\n\n        # Class token for ViT, used to aggregate global information from all patches\n        self.cls_token = vit.cls_token\n\n        # Compute the total embedding length, including class token and positional embeddings\n        embed_len = num_patches + vit.num_prefix_tokens\n        # Positional embedding for the patches and class token\n        self.pos_embed = vit.pos_embed\n        # Dropout for positional embeddings to reduce overfitting\n        self.pos_drop = nn.Dropout(p=0.)\n\n        # Subset of ViT transformer blocks up to the specified depth in the configuration\n        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)])\n\n        # GPT-2 style transformer, adapted for vision-language tasks\n        self.transformer = nn.ModuleDict(dict(\n            # Token embeddings for input vocabulary\n            wte=nn.Embedding(config.vocab_size, config.embed_dim),\n            # Positional embeddings for sequence tokens\n            wpe=nn.Embedding(config.seq_len, config.embed_dim),\n            # Dropout for embeddings\n            drop=nn.Dropout(config.emb_dropout),\n            # Transformer blocks for processing sequences\n            h=nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n            # Final layer normalization\n            ln_f=nn.LayerNorm(config.embed_dim)\n        ))\n\n        # Linear layer to project transformer output to vocabulary logits\n        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n        # Share weights between the embedding layer and the language modeling head\n        self.transformer.wte.weight = self.lm_head.weight\n\n    def _pos_embed(self, x):\n        \"\"\"\n        Add positional embeddings to input patches and prepend the class token.\n\n        Args:\n            x (Tensor): Patch embeddings of shape (batch_size, num_patches, embed_dim).\n\n        Returns:\n            Tensor: Input with positional embeddings and class token, shape (batch_size, embed_len, embed_dim).\n        \"\"\"\n        # Retrieve positional embeddings\n        pos_embed = self.pos_embed\n        # Prepend class token to patch embeddings\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        # Add positional embeddings\n        x = x + pos_embed\n        # Apply dropout\n        return self.pos_drop(x)\n\n    def pretrained_layers_trainable(self, trainable=False):\n        \"\"\"\n        Toggle trainability of pretrained layers (ViT and GPT-2).\n\n        Args:\n            trainable (bool): Whether to make layers trainable (True) or freeze them (False).\n        \"\"\"\n        # List of all layers in the model\n        layers = [\n            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n            self.transformer.wte, self.transformer.wpe,\n            self.transformer.ln_f, self.lm_head\n        ]\n\n        # Add GPT-2 transformer layers to the list\n        gpt_layers = [[\n            self.transformer.h[i].ln_1, self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn, self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        for l in gpt_layers:\n            layers.extend(l)\n\n        # Set the requires_grad property for each layer's parameters\n        for layer in layers:\n            if not isinstance(layer, nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = trainable\n            else:\n                layer.requires_grad = trainable\n\n        # Calculate and print the number of frozen parameters\n        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n        print(f'{total_frozen_params=}')\n\n    def unfreeze_gpt_layers(self):\n        \"\"\"\n        Unfreeze all GPT-2 transformer layers, enabling them to be trainable.\n        \"\"\"\n        # Flatten all GPT-2 layers for easy iteration\n        gpt_layers = [[\n            self.transformer.h[i].ln_1, self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn, self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        flatten = []\n        for l in gpt_layers:\n            flatten.extend(l)\n\n        # Set the requires_grad property for each parameter in GPT-2 layers\n        for layer in flatten:\n            if not isinstance(layer, nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = True\n            else:\n                layer.requires_grad = True\n\n    # The @classmethod decorator in Python is used to define a method that belongs to the class itself rather than to a specific instance of the class. \n    # This means the method can be called on the class directly without needing to instantiate an object of the class.  \n    @classmethod\n    def from_pretrained(cls, config):\n        \"\"\"\n        Load a pre-trained VisionGPT2Model by combining ViT and GPT-2 pretrained weights.\n\n        Args:\n            config (object): Configuration object containing model parameters.\n\n        Returns:\n            VisionGPT2Model: A VisionGPT2Model instance initialized with pretrained weights.\n        \"\"\"\n        # Initialize a new model instance\n        model = cls(config)\n        sd = model.state_dict()  # Model's current state dictionary\n        keys = sd.keys()  # All keys in the state dictionary\n\n        # Define patterns to identify which weights to ignore\n        ignore_matches = ['blocks.', 'cross_attn.', 'ln_3', 'cls_token', 'pos_embed', 'patch_embed.', '.attn.mask']\n\n        # Separate ViT-specific keys\n        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n        # Remaining keys are considered GPT-specific\n        gpt_keys = [key for key in keys if key not in vit_keys]\n\n        # Load pretrained GPT-2 weights\n        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n        sd_hf = gpt2_small.state_dict()  # GPT-2's state dictionary\n        hf_keys = sd_hf.keys()  # All GPT-2 keys\n\n        # Filter GPT-2 keys to exclude unnecessary masked bias and bias keys\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n\n        # Define keys that need transposing for weight shape compatibility\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n\n        # Copy relevant weights from GPT-2 to the model\n        for k in hf_keys:\n            if any(match in k for match in ignore_matches):\n                continue  # Skip ignored keys\n            if any(k.endswith(w) for w in transposed):  # Transpose weights if needed\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:  # Direct copy for matching shapes\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        # Load updated state dictionary into the model\n        model.load_state_dict(sd)\n\n        return model\n\n    def forward(self, image, input_ids, labels=None):\n        \"\"\"\n        Forward pass for vision-language processing.\n\n        Args:\n            image (Tensor): Input image tensor.\n            input_ids (Tensor): Sequence of token IDs.\n            labels (Tensor, optional): Ground truth labels for loss computation.\n\n        Returns:\n            Tensor: Loss value if labels are provided, else logits of the final token.\n        \"\"\"\n        # Extract patch embeddings from the input image\n        image = self.patch_embed(image)\n        image = self._pos_embed(image)\n\n        # Get token embeddings and positional embeddings\n        token_embeddings = self.transformer.wte(input_ids)  # Token embeddings\n        pos_embs = torch.arange(0, input_ids.size(1)).to(input_ids.device)\n        positional_embeddings = self.transformer.wpe(pos_embs)  # Positional embeddings\n        input_ids = self.transformer.drop(token_embeddings + positional_embeddings)  # Combine with dropout\n\n        # Process both image and input tokens through transformer layers\n        for i in range(self.config.depth):\n            image = self.blocks[i](image)  # Process image embeddings through ViT blocks\n            input_ids = self.transformer.h[i](input_ids, image)  # Fuse image and token embeddings\n\n        # Apply final layer normalization\n        input_ids = self.transformer.ln_f(input_ids)\n\n        # Compute loss if labels are provided\n        if labels is not None:\n            lm_logits = self.lm_head(input_ids)  # Project to vocabulary space\n            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n            return loss\n\n        # Return logits of the last token if no labels are provided\n        lm_logits = self.lm_head(input_ids[:, [-1], :])\n        return lm_logits\n\n    def generate(self, image, sequence, max_tokens=50, temperature=1.0, deterministic=False):\n        \"\"\"\n        Generate a sequence of tokens using autoregressive decoding.\n\n        Args:\n            image (Tensor): Input image tensor.\n            sequence (Tensor): Initial sequence of token IDs.\n            max_tokens (int): Maximum number of tokens to generate.\n            temperature (float): Sampling temperature to control randomness.\n            deterministic (bool): Use deterministic (greedy) sampling if True.\n\n        Returns:\n            Tensor: Generated token sequence.\n        \"\"\"\n        for _ in range(max_tokens):\n            # Predict the next token\n            out = self(image, sequence)\n            out = out[:, -1, :] / temperature  # Normalize logits by temperature\n            probs = F.softmax(out, dim=-1)  # Convert logits to probabilities\n\n            # Select the next token deterministically or probabilistically\n            if deterministic:\n                next_token = torch.argmax(probs, dim=-1, keepdim=True)\n            else:\n                next_token = torch.multinomial(probs, num_samples=1)\n\n            # Append the predicted token to the sequence\n            sequence = torch.cat([sequence, next_token], dim=1)\n\n            # Stop generation if the end-of-sequence token is generated\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n        return sequence.cpu().flatten()\n\nclass Trainer:\n    def __init__(self, model_config, train_config, dls):\n        \"\"\"\n        Initialize the Trainer with configurations and data loaders.\n\n        Args:\n            model_config (SimpleNamespace): Configuration for the VisionGPT2Model.\n            train_config (SimpleNamespace): Training configurations, including epochs, learning rate, etc.\n            dls (tuple): A tuple containing training and validation dataloaders.\n\n        Attributes:\n            model: The VisionGPT2Model initialized with pretrained weights.\n            tokenizer: GPT2 tokenizer for text processing.\n            scaler: Gradient scaler for mixed-precision training.\n            train_dl, val_dl: Training and validation dataloaders.\n            optim: Adam optimizer for the model.\n            sched: OneCycleLR scheduler for learning rate scheduling.\n            metrics: DataFrame to track training and validation losses and perplexities.\n            gen_tfms: Transformations for image preprocessing during caption generation.\n        \"\"\"\n        self.train_config = train_config\n        self.model_config = model_config\n        self.device = self.train_config.device\n\n        # Load and prepare the model\n        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n        self.model.pretrained_layers_trainable(trainable=False)\n\n        print(f'Trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n\n        # Prepare tokenizer\n        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        # Gradient scaler for mixed precision\n        self.scaler = GradScaler()\n\n        # Data loaders\n        self.train_dl, self.val_dl = dls\n\n        # Optimizer and scheduler\n        total_steps = len(self.train_dl)\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.0)\n        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n            self.optim,\n            max_lr=self.train_config.lr,\n            epochs=self.train_config.epochs,\n            steps_per_epoch=total_steps\n        )\n\n        # Metrics DataFrame\n        self.metrics = pd.DataFrame()\n        self.metrics[['train_loss', 'train_perplexity', 'val_loss', 'val_perplexity']] = None\n\n        # Image preprocessing for caption generation\n        self.gen_tfms = A.Compose([\n            A.Resize(224, 224),\n            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n            ToTensorV2()\n        ])\n\n    def save_model(self):\n        \"\"\"Save the current model state to a file.\"\"\"\n        self.train_config.model_path.mkdir(exist_ok=True)\n        sd = self.model.state_dict()\n        torch.save(sd, self.train_config.model_path / 'captioner.pt')\n\n    def load_best_model(self):\n        \"\"\"Load the best saved model from file.\"\"\"\n        sd = torch.load(self.train_config.model_path / 'captioner.pt')\n        self.model.load_state_dict(sd)\n\n    def train_one_epoch(self, epoch):\n        \"\"\"\n        Train the model for one epoch.\n\n        Args:\n            epoch (int): Current epoch number.\n        \"\"\"\n        prog = tqdm(self.train_dl, total=len(self.train_dl))\n        running_loss = 0.0\n\n        for image, input_ids, labels in prog:\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image, input_ids, labels)\n\n                # Backpropagation and optimization\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optim)\n                nn.utils.clip_grad_norm_(self.model.parameters(), self.train_config.gradient_clip)\n                self.scaler.update()\n                self.sched.step()\n                self.optim.zero_grad(set_to_none=True)\n\n                running_loss += loss.item()\n                prog.set_description(f'Train loss: {loss.item():.3f}')\n\n            # Clean up to save memory\n            del image, input_ids, labels, loss\n\n        # Compute average loss and perplexity\n        train_loss = running_loss / len(self.train_dl)\n        train_pxp = np.exp(train_loss)\n        self.metrics.loc[epoch, ['train_loss', 'train_perplexity']] = (train_loss, train_pxp)\n\n    @torch.no_grad()\n    def valid_one_epoch(self, epoch):\n        \"\"\"\n        Validate the model for one epoch.\n\n        Args:\n            epoch (int): Current epoch number.\n\n        Returns:\n            float: Validation perplexity.\n        \"\"\"\n        prog = tqdm(self.val_dl, total=len(self.val_dl))\n        running_loss = 0.0\n\n        for image, input_ids, labels in prog:\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image, input_ids, labels)\n                running_loss += loss.item()\n                prog.set_description(f'Valid loss: {loss.item():.3f}')\n\n            # Clean up to save memory\n            del image, input_ids, labels, loss\n\n        # Compute average loss and perplexity\n        val_loss = running_loss / len(self.val_dl)\n        val_pxp = np.exp(val_loss)\n        self.metrics.loc[epoch, ['val_loss', 'val_perplexity']] = (val_loss, val_pxp)\n        return val_pxp\n\n    def clean(self):\n        \"\"\"Perform garbage collection and free CUDA memory.\"\"\"\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def fit(self):\n        \"\"\"\n        Train and validate the model for multiple epochs.\n\n        Returns:\n            dict: Best perplexity and corresponding epoch.\n        \"\"\"\n        best_pxp = 1e9\n        best_epoch = -1\n        prog = tqdm(range(self.train_config.epochs))\n\n        for epoch in prog:\n            if epoch == self.train_config.freeze_epochs_gpt:\n                self.model.unfreeze_gpt_layers()\n                print('Unfreezing GPT2 entirely...')\n\n            if epoch == self.train_config.freeze_epochs_all:\n                self.model.pretrained_layers_trainable(trainable=True)\n\n            # Training phase\n            self.model.train()\n            prog.set_description('Training')\n            self.train_one_epoch(epoch)\n            self.clean()\n\n            # Validation phase\n            self.model.eval()\n            prog.set_description('Validating')\n            pxp = self.valid_one_epoch(epoch)\n            self.clean()\n\n            print(self.metrics.tail(1))\n\n            # Save the best model\n            if pxp < best_pxp:\n                best_pxp = pxp\n                best_epoch = epoch\n                print('Saving best model...')\n                self.save_model()\n\n        return {\n            'best_perplexity': best_pxp,\n            'best_epoch': best_epoch\n        }\n\n    @torch.no_grad()\n    def generate_caption(self, image, max_tokens=50, temperature=1.0, deterministic=False):\n        \"\"\"\n        Generate a caption for the given image.\n\n        Args:\n            image (str): Path to the input image.\n            max_tokens (int): Maximum number of tokens to generate.\n            temperature (float): Sampling temperature.\n            deterministic (bool): If True, use deterministic generation.\n\n        Returns:\n            str: Generated caption.\n        \"\"\"\n        self.model.eval()\n\n        # Preprocess the image\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        image = self.gen_tfms(image=image)['image']\n        image = image.unsqueeze(0).to(self.device)\n\n        # Prepare initial sequence\n        sequence = torch.ones(1, 1).to(device=self.device).long() * self.tokenizer.bos_token_id\n\n        # Generate caption\n        caption = self.model.generate(\n            image,\n            sequence,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            deterministic=deterministic\n        )\n        caption = self.tokenizer.decode(caption.numpy(), skip_special_tokens=True)\n\n        return caption\n\n\n# Define model configuration using SimpleNamespace for easy attribute access\nmodel_config = SimpleNamespace(\n    vocab_size=50_257,            # Size of the vocabulary (e.g., GPT-2's vocabulary size)\n    embed_dim=768,                # Embedding dimension for each token\n    num_heads=12,                 # Number of attention heads in each transformer block\n    seq_len=1024,                 # Maximum sequence length supported by the model\n    depth=12,                     # Number of transformer layers in the model\n    attention_dropout=0.1,        # Dropout rate for the attention mechanism\n    residual_dropout=0.1,         # Dropout rate for residual connections\n    mlp_ratio=4,                  # Ratio for the feed-forward layer's hidden size to input size\n    mlp_dropout=0.1,              # Dropout rate in the feed-forward layer\n    emb_dropout=0.1               # Dropout rate for token embeddings\n)\n\n# Define training configuration using SimpleNamespace\ntrain_config = SimpleNamespace(\n    epochs=5,                     # Total number of training epochs\n    freeze_epochs_gpt=1,          # Number of epochs to freeze GPT layers during training\n    freeze_epochs_all=2,          # Number of epochs to freeze all pretrained layers\n    lr=1e-4,                      # Initial learning rate for training\n    device='cuda',                # Device to use for training ('cuda' for GPU or 'cpu' for CPU)\n    model_path=Path('captioner'), # Path to save the trained model\n    batch_size=32                 # Batch size for training and validation\n    gradient_clip=1.0,  # Clip gradients to this value\n)\n\n# Create DataLoader for the training dataset\ntrain_dl = torch.utils.data.DataLoader(\n    train_ds,                      # Training dataset\n    batch_size=train_config.batch_size, # Batch size for training\n    shuffle=True,                  # Shuffle data at every epoch\n    pin_memory=True,               # Pin memory for faster data transfer to GPU\n    num_workers=2,                 # Number of workers for data loading\n    persistent_workers=True,       # Keep workers alive across epochs for efficiency\n    collate_fn=collate_fn          # Custom function to process and batch data\n)\n\n# Create DataLoader for the validation dataset\nval_dl = torch.utils.data.DataLoader(\n    val_ds,                        # Validation dataset\n    batch_size=train_config.batch_size, # Batch size for validation\n    shuffle=False,                 # Do not shuffle validation data\n    pin_memory=True,               # Pin memory for faster data transfer to GPU\n    num_workers=2,                 # Number of workers for data loading\n    persistent_workers=True,       # Keep workers alive across epochs for efficiency\n    collate_fn=collate_fn          # Custom function to process and batch data\n)\n\n# Initialize the Trainer class with model configuration, training configuration, and DataLoaders\ntrainer = Trainer(model_config, train_config, (train_dl, val_dl))\n\n# Train the model and fit it on the dataset\ntrainer.fit()\n\n# View the metrics collected during training and validation\ntrainer.metrics\n\n# Plot training and validation loss\nplt.plot(trainer.metrics['train_loss'], color='red', label='train loss')  # Plot train loss\nplt.plot(trainer.metrics['val_loss'], color='orange', label='valid loss')  # Plot validation loss\nplt.title('Loss, lower=better')  # Title for the plot\nplt.legend()  # Add legend to differentiate curves\nplt.show()  # Display the plot\n\n# Plot training and validation perplexity\nplt.plot(trainer.metrics['train_perplexity'], color='blue', label='train perplexity')  # Train perplexity\nplt.plot(trainer.metrics['val_perplexity'], color='lightblue', label='valid perplexity')  # Validation perplexity\nplt.title('Perplexity, lower=better')  # Title for the plot\nplt.legend()  # Add legend to differentiate curves\nplt.show()  # Display the plot\n\n# Load the best model based on validation performance\ntrainer.load_best_model()\n\n# Test the model by generating captions for random samples from the validation dataset\nfor i in range(50):  # Generate captions for 50 samples\n    det = False  # Deterministic generation flag, initially set to False\n    test = val_df.sample(n=1).values[0]  # Randomly sample one validation data point\n    test_img, test_caption = test[0], test[1]  # Extract image path and actual caption\n\n    # Display the test image\n    plt.imshow(Image.open(test_img).convert('RGB'))  # Open and convert the image to RGB format\n\n    # Generate a random temperature for diversity in generation\n    t = np.random.uniform(0.5, 1.5)\n\n    # Use deterministic generation for the last 10 samples\n    if i > 40:\n        det = True\n\n    # Generate a caption for the test image\n    gen_caption = trainer.generate_caption(test_img, temperature=t, deterministic=det)\n\n    # Display the actual and generated captions along with generation parameters\n    plt.title(f\"actual: {test_caption}\\nmodel: {gen_caption}\\ntemp: {t} deterministic generation: {det}\")\n    plt.axis('off')  # Remove axis for better visual appeal\n    plt.show()  # Show the image and captions","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}